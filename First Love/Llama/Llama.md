*The Llama 3 Herd of Models
1.Multilinguality coding reasoning tool uasage
2.dense Transformer 405B para 128K tokens context window instead of MoE
3.Guard 3 for input and output safety
4.image video speech via compotional approach

**Introduction
***modern foundation 
(1)pre-training = next-word prediction//captioning at massive scale (GPT 1 2 3)
(2)post-training = follow instructions with hunman preference and improve specific capabilities(instruct GPT/RLHF)

***3 keys
(1)Data 15T tokens
(2)Scale 405B
(3)Managing complexing(feature/bug)  a standard dense Transformer(SFT) (RS) (DPO)


![image](https://github.com/user-attachments/assets/fd45cb34-d415-461f-94e5-8c6f6567e0ec)
